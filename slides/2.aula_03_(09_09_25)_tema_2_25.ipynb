{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "<left><img src=\"https://i.ibb.co/zWjkHsWJ/marca-final-rgb-campanha-2025-versao02.png\" width=\"35%\" height=\"35%\"></left>"
      ],
      "metadata": {
        "id": "LmiSwTbnVRkJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tópicos em Matemática Aplicada: Deep Learning (Aula 03)\n",
        "\n",
        "Data: 09/set/25"
      ],
      "metadata": {
        "id": "rKJbDcsngBrH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Introdução (Exemplo 1)"
      ],
      "metadata": {
        "id": "kN2yRrb2AM7S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Objetivo**: construir uma rede neural para classificar dígitos manuscritos da base de dados MNIST.\n",
        "\n",
        "* Os dígitos são imagens em escala de cinza com dimensão $28\\times 28$ e distribuídas em 10 classes ($0,1,2,\\ldots,9$).\n",
        "\n",
        "* Imagens em escala de cinza são matrizes e cada entrada dela equivale a um nível de intensidade luminosa. Neste conjunto de dados, as imagens variam a intensidade luminosa de $0$ (preto) a $255$ (branco) apenas com entradas inteiras.\n",
        "\n",
        "* Será então construída uma rede neural cuja entrada são as $784=28\\times 28$ entradas das figuras e a saída é a classe em que esta imagem deve estar contida. Neste tipo de situação as matrizes que representam as imagens serão reescritas como um vetor, de tal forma que as entradas do vetor correspondem as colunas da matriz da imagem.\n",
        "\n",
        "* Esta rede terá duas camadas, além da entrada, com 10 neurônios, na primeira utilizaremos a função de ativação `ReLU` e na segunda camada a função `softmax`.\n",
        "\n",
        "* A classe numérica em que cada imagem está contida deverá ser convertida para o que se chama de `formato categórico`, isto é, a cada classe será associado um vetor em $\\mathbb{R}^{10}.$ Por exemplo, a classe $0$ será associada ao vetor $(1,0,0,\\ldots,0),$ a classe $1$ ao vetor $(0,1,0,\\ldots,0)$ e assim por diante. Isto é necessário para podermos comparar adequadamente a saída da rede com a classe, pois estamos calculando distância entre vetores.\n",
        "\n",
        "* Aqui utilizaremos como função de perda a entropia cruzada (*cross-entropy)* $$\\mathcal{L}(y,\\hat{y}_\\theta)=-\\sum_{j=1}^{10} y_j\\ln\\hat{y}_{\\theta,j},$$ sendo $y_j$ a classe original do dígito, $\\hat{y}_{\\theta,j}$ a classe estimada pela rede e $j$ é o índice das imagens utilizadas no treinamento.\n",
        "\n",
        "* A saída que será produzida pela rede neural é um vetor de probabilidades, uma vez que a função de ativação escolhida para a saída é a `softmax`. Isto é, dada uma imagem $\\mathbf{x}$ de entrada, que deverá ser classificada, a rede produzirá como resposta, por exemplo, um vetor na forma $\\hat{y}_{\\theta,j}(\\mathbf{x})=(0.05 \\, ,\\,0.07 \\, ,\\, 0.01\\, ,\\, 0.12\\, ,\\, 0.57\\, ,\\, 0.02\\, ,\\, 0.015\\, ,\\, 0.015\\, ,\\, 0.05\\, ,\\, 0.08).$ Nesta situação, a imagem $\\mathbf{x}$ será classificada como um $4$."
      ],
      "metadata": {
        "id": "XpDTOK6YgkJo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Parte 1: manipulação e visualização dos dados"
      ],
      "metadata": {
        "id": "d4KDxgiWqT4K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "**Iniciamos importando as bibliotecas necessárias!**\n",
        "\n",
        "`numpy` para cálculo numérico\n",
        "\n",
        "`panda` para manipulação de dados\n",
        "\n",
        "`pyplot` para imprimir as figuras"
      ],
      "metadata": {
        "id": "eEFdBfy7aE3X"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pTyOvdfQZNb_"
      },
      "outputs": [],
      "source": [
        "# bibliotecas básicas\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from matplotlib import pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui estamos lendo o arquivo que contém a informação das imagens.\n",
        "\n",
        "*obs: caso esteja utilizando o `google colab` espere carregar todo o arquivo `2.aula_03_(09_09_25)_data_tema_2_25.csv`, caso contrário a rede dará algum erro e não convergirá adequadamente.*"
      ],
      "metadata": {
        "id": "5axnyzSSe-gL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# acessa o google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# le o arquivo das imagens\n",
        "data = pd.read_csv('/content/drive/MyDrive/TEMA - 2.25/códigos/2.aula_03_(09_09_25)_data_tema_2_25.csv')"
      ],
      "metadata": {
        "id": "tQtT5Ubxa74_",
        "outputId": "1592dcf2-1712-4832-9944-37e7876a1ad2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Estamos lidando com 42000 amostras de imagens de manuscritos de tamanho $28\\times 28=784$ já escritos no formato de vetor.\n",
        "\n",
        "Perceba que o vetor tem uma entrada a mais. Ela representa a etiqueta daquele número manuscrito."
      ],
      "metadata": {
        "id": "NsZ_800TbI5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = np.array(data)\n",
        "m, n = data.shape # método que devolve a dimensão dos dados\n",
        "print(data.shape)"
      ],
      "metadata": {
        "id": "cLU_FlRZfNYZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b2d2d1-95bb-4172-dc70-c45771c6e056"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(42000, 785)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Das 42000 imagens, separamos 4200 (10%) delas pra teste e as demais para o treino!"
      ],
      "metadata": {
        "id": "Kfme66_2b64M"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#separar o que é teste e o que é treino\n",
        "\n",
        "np.random.shuffle(data) # começamos embaralhando os dados com o método shuffle\n",
        "\n",
        "#teste\n",
        "data_dev = data[0:4200].T\n",
        "Y_dev = data_dev[0] # classe\n",
        "X_dev = data_dev[1:n] # a informação das imagens\n",
        "\n",
        "#treino\n",
        "data_train = data[4200:m].T\n",
        "Y_train = data_train[0]\n",
        "X_train = data_train[1:n]\n",
        "_,m_train = X_train.shape"
      ],
      "metadata": {
        "id": "9S0NRqaCfn1i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Vamos analisar como são as imagens. Perceba que nas matrizes os valores são inteiros e variam entre $0$ e $255$."
      ],
      "metadata": {
        "id": "-fjuof-7nnQW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#mostra as imagens\n",
        "Z = X_train.reshape((28,28,m_train)) # o método reshape serve para transformar o vetor empilhado de volta numa matriz\n",
        "\n",
        "print(Z[:,:,0]) # começamos mostrando como são os dados das imagens"
      ],
      "metadata": {
        "id": "3cNQFv8QgrIX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5150298b-019c-4115-905d-d20b1a2d1e8f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0  30   0   0   0   0   0   0   0  28 143 253 255\n",
            "  146   3   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 144   4   0   0   0   0   0  80 225 214 148 190\n",
            "  254  74   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 210  32   0   0   0   0  44 245 192  22   0 102\n",
            "  254 111   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 210  75   4   0   0  15 214 211  28   0   0 102\n",
            "  248  53   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 210 254 135   0   0 119 244  67   0   0   0 121\n",
            "  176   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 210 164 243  30  15 249 213   0   0   0   3 202\n",
            "  140   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0 125  11  56  31  70 254 135   0   0   0  89 254\n",
            "  114   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  92 254 132   0   0  65 228 209\n",
            "   18   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0  17 254 172  29 122 250 254 152\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   4 147 245 254 230 174 254 115\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0  32  40  17 136 252  58\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  38 247 173   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  65 254 165   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  94 254  86   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 152 254  52   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 226 254   4   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 212 254  18   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0 128 254 128   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0  48 250 187   0\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0 110 253 146\n",
            "    0   0   0   0   0   0   0   0   0   0]\n",
            " [  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
            "    0   0   0   0   0   0   0   0   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Agora vemos como os dígitos manuscritos são visualmente."
      ],
      "metadata": {
        "id": "GZBckKt9oAEf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fig, ax = plt.subplots(1, 1, figsize=(4,4))\n",
        "\n",
        "ax.imshow(Z[:,:,0],cmap='gray') # plota a imagem\n",
        "ax.set_title(f\"Rótulo: {Y_train[0]}\") # inclui o rótulo em que essa imagem está classificada\n",
        "ax.get_xaxis().set_visible(False) # tiram os eixos x e y da visualização\n",
        "ax.get_yaxis().set_visible(False)"
      ],
      "metadata": {
        "id": "yL-TrJKjn_d0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 387
        },
        "outputId": "c1720f71-9acd-463f-e6bf-df34006a6a8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 400x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAUgAAAFeCAYAAADnm4a1AAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAAD8ZJREFUeJzt3X2sl3X9x/H3F7kNzzmGsBY3YrFMRrk0xIIgLLNAqZOuUzbWKrDZDJcFNfAPzsk2b5o3qVCutWDQZFZLTbEp3YxNSK2FxUq60ZMlFTLkwNQTHrh+fzTOfnR4y+cc+crh8Hhs/uE5r3Pxgbmn17m5+NaqqqoCgB4GHesDAPRXAgmQEEiAhEACJAQSICGQAAmBBEgIJEBCIOmzXbt2RVtbWzz66KPH+ihQFwJJn1RVFZ/61Kfil7/8ZZx99tlH/fqrVq2KWq0W7e3tR/3aUEogT3AHQ3Twn8GDB8e4cePi05/+dDz77LPpx914443R3t4eP/7xj2Po0KGHvG/Tpk3R2toau3fvrvPpj45169bFOeecE8OHD48xY8bEggULYufOncf6WPQDAklERHzta1+LNWvWxLe//e2YM2dOrF27Nt773vdGZ2dnj21nZ2d0dXXF+vXr45RTTunx/k2bNkVbW9txEchvfetbcdlll8WoUaPi5ptvjssvvzzWrVsX73//+w/7e+fEMvhYH4D+Yc6cOTF16tSIiFi4cGGMHj06brjhhrjvvvuipaXlkO3w4cPjmmuuORbHPKr27dsXy5Yti1mzZsXDDz8ctVotIiKmT58e8+bNi+985zuxaNGiY3xKjiV3kBzWzJkzIyLir3/96yFv//nPfx4zZ86MkSNHximnnBIf+chH4o9//GP3+1tbW2PJkiUREfGmN72p+1P39vb2aG9vj1qtFqtWrerx69VqtWhtbT3iuVauXBlTpkyJYcOGxdixY+PKK6/scaf64osvxpNPPnnET5O3bt0au3fvjo9//OPdcYyIuPjii+Pkk0+OdevWHfE8DGwCyWEd/ObI61//+u63bdiwIT74wQ/Gjh07orW1Nb70pS/Fpk2bYsaMGd37Sy65JC677LKIiLjllltizZo1sWbNmhgzZsyrPlNra2tceeWVMXbs2Ljpppvi0ksvjTvvvDMuvPDCePnll7t3jz32WEyePDnuuOOOV7zef/7zn4iIGDFiRI/3jRgxIn7729/GgQMHXvW5OX75FJuIiOjo6IidO3dGZ2dnPProo9HW1hbDhg2Liy++uHuzZMmSGDVqVGzevDlGjRoVERHNzc1x9tlnx/Lly2P16tVx1llnxTnnnBN33XVXNDc3x+mnn9798c8991yfz/fcc8/FddddFxdeeGE8+OCDMWjQf//ffuaZZ8YXvvCFWLt2bXzmM5/p1TXf8pa3RK1Wi0ceeeSQj922bVv3WZ9//vk49dRT+3xujm8CSUREXHDBBYf8++mnnx5r166N8ePHR0TEP//5z9iyZUt85Stf6Y5jRMRZZ50VH/jAB2L9+vV1Pd+GDRti37598cUvfrE7jhERl19+eSxbtiweeOCB7sjNnj07Sv4e6NGjR0dLS0usXr06Jk+eHB/96Efj2WefjUWLFsWQIUPi5Zdfjpdeeqluvyf6P59iExERK1asiIcffjh++MMfxty5c2Pnzp0xbNiw7vf/7W9/i4iIt771rT0+dvLkybFz58544YUX6na+7NcfOnRovPnNb+5+f2/deeedMXfu3Fi8eHFMmjQpZs2aFW9/+9tj3rx5ERFx8sknv7qDc1xzB0lEREybNq37u9jNzc3xnve8Jz75yU/Gtm3bjlok/v83Qv6//fv3H5Xr90VTU1Pce++98cwzz0R7e3tMnDgxJk6cGNOnT48xY8Yc9seYOHG4g6SHk046Ka677rrYvn179zc6Jk6cGBH//frc/3ryySdj9OjRMXLkyIjIQ3jwGz7/+13nkru/7Nfft29fPP30093v76vTTjstZs2aFRMnTozdu3fHb37zmx5fduDEI5Ac1uzZs2PatGlx6623RmdnZ7zxjW+Md7zjHbF69epDArd169Z46KGHYu7cud1vOxjK/w1hY2NjjB49OjZu3HjI21euXHnE81xwwQUxdOjQuO222w75+uJ3v/vd6OjoiIsuuqj7baU/5pNZunRpdHV1xdVXX92nj2fg8Ck2qSVLlsTHPvaxWLVqVVxxxRXxjW98I+bMmRPvfve7Y8GCBfHSSy/F7bffHk1NTYf8DOM73/nOiIi45ppr4hOf+EQMGTIk5s2bFyNHjoyFCxfG9ddfHwsXLoypU6fGxo0b409/+tMRzzJmzJhYunRptLW1xYc+9KH48Ic/HNu2bYuVK1fGueeeG/Pnz+/ePvbYY3H++efH8uXLj/izlddff31s3bo1zjvvvBg8eHDcc8898dBDD8XXv/71OPfcc/v058YAUnFC+973vldFRPX444/3eN/+/furSZMmVZMmTaq6urqqqqqqDRs2VDNmzKhGjBhRNTY2VvPmzav+8Ic/9PjYa6+9tho3blw1aNCgKiKqp59+uqqqqnrxxRerBQsWVE1NTVVDQ0PV0tJS7dixo4qIavny5T3OdfDjDrrjjjuqM888sxoyZEj1hje8ofr85z9fPf/884dsfvGLX/S4Xub++++vpk2bVjU0NFSve93rqne9613V3XfffcSP48RQqyqviw1wOL4GCZAQSICEQAIkBBIgIZAACYEESBT9oPiBAwdi+/bt0dDQkD5GBnA8qKoq9u7dG2PHjj3kb4Y6nKJAbt++PSZMmHBUDgfQH/z973/v/uv8MkWfYjc0NByVAwH0FyVdKwqkT6uBgaaka75JA5AQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZAQSIDE4GN9gOPNaaedVrx95pln6niSgWn8+PHF2y9/+cvF26uuuqp4O2hQ+X3D7bffXry9+uqri7f79+8v3lI/7iABEgIJkBBIgIRAAiQEEiAhkAAJgQRICCRAQiABEgIJkKhVVVUdabRnz55oamp6Lc7T7916663F28WLFxdvu7q6+nCa40Nzc3Px9pvf/Gbx9oUXXije/uxnPyveTpkypXg7e/bs4u1FF11UvH3wwQeLt/RNR0dHNDY2vuLGHSRAQiABEgIJkBBIgIRAAiQEEiAhkAAJgQRICCRAQiABEl7VsJfe9773FW978wp97e3tfTjNsfO2t72tePv973+/ePv4448Xb1taWoq3O3bsKN5+7nOfK9725lHDM844o3jrUcP+wR0kQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARIeNayj3rw63j/+8Y/ibb1eAbE3r1x59913F2+feuqp4u38+fOLt715fLBe9u3bV7x95JFH6ngS6sEdJEBCIAESAgmQEEiAhEACJAQSICGQAAmBBEgIJEBCIAESHjWso5/85CfF2xtuuKF4u3Tp0r4c54iuuOKK4u2ECROKt83NzcXb3jxyWS+nnnpq8fanP/1p8fbXv/51X47DMeQOEiAhkAAJgQRICCRAQiABEgIJkBBIgIRAAiQEEiAhkACJWlVV1ZFGe/bs6dUr3g1kv/vd74q3TzzxRPG2N4/j9ebVEjs6Ooq3vXmVwL/85S/F296ct15OOumk4u3mzZuLtxs3bizeLl68uHhL/XV0dERjY+MrbtxBAiQEEiAhkAAJgQRICCRAQiABEgIJkBBIgIRAAiQEEiDhVQ176Qc/+EHx9uabby7erlixonhbq9WKt+edd17xdsiQIcXb++67r3jbH7S0tBRvp06dWry96667+nIcjhPuIAESAgmQEEiAhEACJAQSICGQAAmBBEgIJEBCIAESAgmQ8KhhL1177bV1ue6vfvWrulx35syZdbluV1dXXa7bG+eff37x9sYbbyze/v73vy/erl69unjL8ccdJEBCIAESAgmQEEiAhEACJAQSICGQAAmBBEgIJEBCIAESHjUc4B544IG6XHfy5MnF2wkTJhRv58+fX7xta2sr3g4eXP6fem/+zHbt2lW85fjjDhIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIgIZAACY8aDnB79+4t3q5YsaJ4+9nPfrZ4e8kllxRv//WvfxVv77///uJtc3Nz8fbPf/5z8ZaBzR0kQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARIeNRzgurq6ireLFi0q3t50003F2+HDhxdv//3vfxdvv/rVrxZvDxw4ULzdvHlz8ZaBzR0kQEIgARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARK1qqqqI4327NkTTU1Nr8V5OMGdccYZxdsnnniieLt+/fri7aWXXlq85fjV0dERjY2Nr7hxBwmQEEiAhEACJAQSICGQAAmBBEgIJEBCIAESAgmQEEiAhFc1pF8ZN25c8XbYsGHF287Ozr4chxOcO0iAhEACJAQSICGQAAmBBEgIJEBCIAESAgmQEEiAhEACJDxqSL8yYcKEulz33nvvrct1GdjcQQIkBBIgIZAACYEESAgkQEIgARICCZAQSICEQAIkBBIg4VFD+pXp06fX5bpPPfVUXa7LwOYOEiAhkAAJgQRICCRAQiABEgIJkBBIgIRAAiQEEiAhkAAJjxrSr8ydO7cu192yZUtdrsvA5g4SICGQAAmBBEgIJEBCIAESAgmQEEiAhEACJAQSICGQAAmPGtKvdHR0FG/Hjx9fvG1sbCze7tq1q3jLwOYOEiAhkAAJgQRICCRAQiABEgIJkBBIgIRAAiQEEiAhkAAJjxrSr/zoRz8q3k6ZMqV4e9VVVxVvW1tbi7cMbO4gARICCZAQSICEQAIkBBIgIZAACYEESAgkQEIgARICCZDwqCH9yj333FO8XbZsWfF2xowZfTgNJzp3kAAJgQRICCRAQiABEgIJkBBIgIRAAiQEEiAhkAAJgQRIeNSQfmXLli3F28WLFxdvb7nlluLtbbfdVrztzaslcvxxBwmQEEiAhEACJAQSICGQAAmBBEgIJEBCIAESAgmQEEiARK2qqupIoz179kRTU9NrcR6A10RHR0c0Nja+4sYdJEBCIAESAgmQEEiAhEACJAQSICGQAAmBBEgIJECiKJAFD9sAHFdKulYUyL17977qwwD0JyVdK3oW+8CBA7F9+/ZoaGiIWq12VA4HcCxUVRV79+6NsWPHxqBBr3yPWBRIgBORb9IAJAQSICGQAAmBBEgIJEBCIAESAgmQ+D/MCu0JkTIXSgAAAABJRU5ErkJggg==\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui iremos dividir as entradas dos vetores de teste e treino por $255$, isto é, estamos fazendo o que é conhecido de normalização. Na prática isto não é uma normalização dos vetores das imagens, porém agora a escala de como a cor é vista irá variar entre $0$ e $1$, e não mais entre $0$ e $255$. Este tipo de cuidado é importante para ajudar a rede a convergir evitando que o gradiente tenha a possibilidade de crescer demais."
      ],
      "metadata": {
        "id": "6hjcpEkwmNqL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_dev = X_dev / 255. #teste\n",
        "X_train = X_train / 255. #treino"
      ],
      "metadata": {
        "id": "C4nPYMw4mNVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Parte 2: construção da rede neural"
      ],
      "metadata": {
        "id": "GT2lkeMQqNkV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui definimos as funções de ativação que serão utilizadas: `ReLU` e `softmax`.\n",
        "\n",
        "Lembrando que:\n",
        "1. $\\mathrm{ReLU}(x)=\\max(0,x)$;\n",
        "2. $\\frac{d}{dx} \\mathrm{ReLU}(x)=1$, se $x>0$, e $\\frac{d}{dx} \\mathrm{ReLU}(x)=0$, se $x<0$;\n",
        "3. `softmax` produz um vetor em que cada entrada é dada por $\\sigma(\\mathbf{z})_j=\\frac{\\exp(z_j)}{\\sum_k \\exp(z_k)}.$"
      ],
      "metadata": {
        "id": "2FSTMEq066qO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definicao das funções de ativação\n",
        "def ReLU(Z):\n",
        "    return np.maximum(Z, 0)\n",
        "\n",
        "def ReLU_deriv(Z):\n",
        "    return Z > 0 # retorna 1 se Z > 0  e retorna 0 se Z < 0\n",
        "\n",
        "def softmax(Z):\n",
        "    A = np.exp(Z) / np.sum(np.exp(Z), axis=0)\n",
        "    return A"
      ],
      "metadata": {
        "id": "IVT23iksiS1X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Construímos aqui as operações nas camadas.\n",
        "\n",
        "1. Temos que $X$ é a matriz que contém as informações de cada imagem. Cada coluna desta matriz corresponde a uma imagem de um dígito. Essa matriz tem dimensão $784\\times m$, $m$ para nós é a quantidade de imagens de treino. Neste caso, estamos usando $m=38800$.\n",
        "\n",
        "2. Daí, fazemos a transformação afim das entradas\n",
        "$Z^{[1]}=W^{[1]}X+b^{[1]},$\n",
        "sendo que a matriz $W^{[1]}$ contém as informações dos pesos que ligam as entradas aos  $10$ neurônios da primeira camada e $b^{[1]}$ corresponde aos viéses de cada neurônio da primeira cadama. O resultado dessa operação fornece um vetor $10\\times m.$\n",
        "\n",
        "3. Em seguida, temos que aplicar a função de ativação para os neurônios da primeira camada fazendo $A^{[1]}=\\mathrm{ReLU}(Z^{[1]}).$ Note que essa operação é feita entrada à entrada da matriz $Z^{[1]}$ e, portanto, a dimensão de $A^{[1]}$ é a mesma de $Z^{[1]}$.\n",
        "\n",
        "4. Para a segunda camada, onde será produzida a saída, repetimos o procedimento descrito no item 2. Fazemos novamente uma transformação afim das entradas. No entanto, aqui as entradas correspondem às saídas dos neurônios anteriores. Portanto, a matriz de pesos aqui precisa ter dimensão menor, no caso $10\\times 10$. Assim, a transformação afim é $Z^{[2]}=W^{[2]}A^{[1]}+b^{[2]}.$ Novamente, aqui $Z^{[2]}$ produzirá uma matriz $10\\times m.$\n",
        "\n",
        "5. Finalmente, precisamos aplicar a função de ativação na transformação afim que ocorreu na segunda camada. Neste caso a transformação será uma `softmax` que corresponderá as probabilidades citadas iniciais. Assim, $A^{[2]}=\\mathrm{softmax}(Z^{[2]}).$"
      ],
      "metadata": {
        "id": "U9Q_zv3Uc0R2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos operar as imagens nas camadas\n",
        "def forward_prop(W1, b1, W2, b2, X):\n",
        "    Z1 = W1.dot(X) + b1\n",
        "    A1 = ReLU(Z1)\n",
        "    Z2 = W2.dot(A1) + b2\n",
        "    A2 = softmax(Z2)\n",
        "    return Z1, A1, Z2, A2"
      ],
      "metadata": {
        "id": "_zl86jqgi4yZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Definindo os parâmetros iniciais: pesos e viéses\n",
        "\n",
        "O método `rand(a,b)` produz uma matriz de dimensões $a\\times b$ com valores aleatórios que são amostrados da distribuição uniforme no intervalo $[0,1]$. Fazendo, por exemplo, `np.random.rand(10, 784) - 0.5` iremos gerar uma matriz $10\\times 784$ com entradas no intervalo $[-0.5\\, , \\, 0.5].$\n",
        "\n",
        "Na prática a escolha inicial de pesos e viéses é feita usando método especializados. Tanto o Keras/TensorFlow como o Keras/Pytorch trazem as opções usuais de inicializadores para os pesos e viéses que variam desde as distribuições uniforme e normal até os inicializadores de Xavier Glorot. Este último, em particular, tem como objetivo inicializar os pesos de modo que a variância das ativações seja a mesma em todas as camadas. Essa variância constante ajuda a evitar que o gradiente exploda ou desapareça.\n",
        "\n",
        "*Referência: X. Glorot, Y. Bengio. Understanding the difficulty of training deep feedforward neural networks. Proceedings of the Thirteenth International Conference on Artificial Intelligence and Statistics, 2010. URL: https://proceedings.mlr.press/v9/glorot10a.html*"
      ],
      "metadata": {
        "id": "O62yXZ4QcRjk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# definição dos parâmetros iniciais\n",
        "def init_params():\n",
        "    W1 = np.random.rand(10, 784) - 0.5\n",
        "    b1 = np.random.rand(10, 1) - 0.5\n",
        "    W2 = np.random.rand(10, 10) - 0.5\n",
        "    b2 = np.random.rand(10, 1) - 0.5\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "mjsAGpM7gwWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Essa parte do código faz a codificação das classes das imagens para o formato categórico. Isto é importante para a comparação do resultado numérico gerado pela rede neural com os vetores de rótulos"
      ],
      "metadata": {
        "id": "lUuA7H3Bc_C3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# faz o one-hot encoding do vetor de rótulos\n",
        "def one_hot(Y): # one-hot encoding\n",
        "    Y = Y.astype(int)\n",
        "    one_hot_Y = np.zeros((Y.size, Y.max() + 1))\n",
        "    one_hot_Y[np.arange(Y.size), Y] = 1\n",
        "    one_hot_Y = one_hot_Y.T\n",
        "    return one_hot_Y"
      ],
      "metadata": {
        "id": "D7LEmfb0jYPU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Parte 2.1: cálculo das derivadas e o *backpropagation*"
      ],
      "metadata": {
        "id": "ucOLPFKsdf95"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "Agora entramos na parte mais complicada que é o cálculo do gradiente para a atualização dos parâmetros.\n",
        "\n",
        "O que pretendemos calcular é a derivada da função de perda com relação a cada parâmetro de peso e viés. Para um classificador `softmax`, usaremos uma função de perda de entropia cruzada:\n",
        "$$\\mathcal{L}(y,\\hat{y}_\\theta) = -\\sum_{i=1}^{10} y_i \\ln(\\hat{y}_{\\theta,i}).$$\n",
        "*Obs: é praxe dividir a função de perda pela quantidade de amostras que estão sendo utilizadas, no caso aqui $m$.*\n",
        "\n",
        "Aqui, $\\hat{y}_\\theta$ é o vetor dos rótulos previstos e ele pode ter uma forma como:\n",
        "$$\\begin{bmatrix} 0.01 \\ 0.02 \\ 0.05 \\ 0.02 \\ 0.80 \\ 0.01 \\ 0.01 \\ 0.00 \\ 0.01 \\ 0.07\\end{bmatrix}.$$\n",
        "\n",
        "O vetor $y$ é a codificação `one-hot` do rótulo do dado de treinamento. Se o rótulo para um exemplo de treinamento for 4, por exemplo, a sua codificação `one-hot` ficaria na forma:\n",
        "$$\\begin{bmatrix} 0 \\ 0 \\ 0 \\ 0 \\ 1 \\ 0 \\ 0 \\ 0 \\ 0 \\ 0 \\ \\end{bmatrix}.$$\n",
        "\n",
        "Observe que na soma $$\\sum_{i=1}^{10} y_i \\ln(\\hat{y}_{\\theta,i}),$$ o valor de $y_i = 0,$ para todos os $i$ exceto o rótulo correto. A função de perda para uma dada amostra,  é apenas o logaritmo da probabilidade dada para a previsão estimada. Em nosso exemplo acima, $$\\mathcal{L}(y,\\hat{y}_\\theta) = -\\ln(\\hat{y}_5) = -\\ln(0.80) \\approx 0.2231.$$ Observe que, quanto mais próxima a probabilidade de predição estiver de 1, mais próxima a perda estará de 0. Conforme a probabilidade se aproxima de 0, a perda se aproxima de $+\\infty$.\n",
        "\n",
        "A minimização da função de perda melhora a precisão do nosso modelo. Como foi possível ver no exemplo, quanto mais perto a rede aproxima dos rótulos originais, mais perto de zero ficará a função de perda. O processo de otimização é feito através da descida do gradiente, em que subtraímos dos parâmetros que queremos otimizar ($W^{[1]}, W^{[2]}, b^{[1]}$ e $b^{[2]}$) uma quantidade proporcional ao gradiente da função de perda naquela variável, isto é:\n",
        "$$W^{[1]} := W^{[1]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}}$$\n",
        "$$ b^{[1]} := b^{[1]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}}$$\n",
        "$$ W^{[2]} := W^{[2]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$$\n",
        "$$ b^{[2]} := b^{[2]} - \\alpha \\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}, $$\n",
        "o parâmetro $\\alpha$ é o que chamamos de taxa de aprendizagem (*learning rate*).\n",
        "\n",
        "Nosso objetivo no *backpropagation* é calcular as derivadas $\\frac{\\partial \\mathcal{L}}{\\partial W^{[1]}},\\frac{\\partial \\mathcal{L}}{\\partial b^{[1]}},\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}$ e $\\frac{\\partial \\mathcal{L}}{\\partial b^{[2]}}.$ Apenas para simplificar, escreveremos essas quantidades como $dW^{[1]}, db^{[1]}, dW^{[2]},$ e $db^{[2]}$. Esses valores são calculados usando a regra da cadeia retrocedendo em nossa rede, começando pelo cálculo de $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}}$, ou $dZ^{[2]}$. Não é imediato, como veremos a seguir, mas essa derivada é dada por:\n",
        "$$dZ^{[2]} = \\left(A^{[2]}-y\\right).$$\n",
        "\n",
        "A partir desta derivada $dZ^{[2]}$, podemos utilizar a regra da cadeia, para $dW^{[2]}$ e $db^{[2]}$. Temos que:\n",
        "$dW^{[2]} = dZ^{[2]} A^{[1]T}$ e $db^{[2]} =  \\Sigma {dZ^{[2]}}.$\n",
        "\n",
        "Então, para calcular $dW^{[1]}$ e $db^{[1]}$, primeiro encontraremos $dZ^{[1]}$, que é dado por:\n",
        "$$dZ^{[1]} = W^{[2]T} dZ^{[2]} .* g^{[1]\\prime} (Z^{[1]}).$$\n",
        "Em seguida, obtemos:\n",
        "$dW^{[1]} = dZ^{[1]} X^{T}$ e $db^{[1]} = \\Sigma {dZ^{[1]}}.$"
      ],
      "metadata": {
        "id": "RiwYCvz1Ku8c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Aqui vamos mostrar que a derivada $\\frac{\\partial \\mathcal{L}}{\\partial Z^{[2]}}=\\left(A^{[2]}-y\\right).$\n",
        "\n",
        "Começamos, então, tirando o $\\ln$ do `softmax` para obter:\n",
        "\\begin{align*}\\ln\\hat{y}_{\\theta,i} & =\\ln\\left(\\frac{e^{z_{i}}}{\\sum_{j=1}^{10}e^{z_{j}}}\\right)\\\\\n",
        "& =\\ln\\left(e^{z_{i}}\\right)-\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)\\\\\n",
        "& =z_{i}-\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)\n",
        "\\end{align*}\n",
        "\n",
        "Em seguida derivamos a equação anterior em relação a variável $z_{k}$ para obtermos:\n",
        "\\begin{align*}\\frac{\\partial\\ln\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\frac{\\partial z_{i}}{\\partial z_{k}}-\\frac{\\partial\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)}{\\partial z_{k}}\\end{align*}\n",
        "\n",
        "Note que o primeiro elemento de lado direito ficará na forma $$\\frac{\\partial z_{i}}{\\partial z_{k}}=\\begin{cases}\n",
        "1 & z_{i}=z_{k}\\\\\n",
        "0 & \\text{caso contrário}\n",
        "\\end{cases}=\\mathbb{1}(z_{i}=z_{k})=\\delta_{ik},$$ em que $\\mathbb{1}(z_{i}=z_{k})$ é a função indicadora, que, neste caso, também pode ser representada pelo $\\delta_{ik}$ delta de Kronecker.\n",
        "\n",
        "A segunda parcela do lado direito pode ser simplificada também, onde obtemos:\n",
        "\\begin{align*}\\frac{\\partial\\ln\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)}{\\partial z_{k}} & =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\frac{\\partial\\left(\\sum_{j=1}^{10}e^{z_{j}}\\right)}{\\partial z_{k}}\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}\\frac{\\partial e^{z_{j}}}{\\partial z_{k}}\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}e^{z_{j}}\\frac{\\partial z_{j}}{\\partial z_{k}}\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}e^{z_{j}}\\mathbb{1}(z_{j}=z_{k})\\\\\n",
        "& =\\frac{1}{\\sum_{j=1}^{10}e^{z_{j}}}\\sum_{j=1}^{10}e^{z_{j}}\\delta_{jk}\\\\\n",
        "& =\\frac{e^{z_{k}}}{\\sum_{j=1}^{10}e^{z_{j}}}\\\\\n",
        "& =\\hat{y}_{\\theta,k}\n",
        "\\end{align*}\n",
        "\n",
        "Concluímos então que:\n",
        "\\begin{align*}\\frac{\\partial\\ln\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\delta_{ik}-\\hat{y}_{\\theta,k}\\\\\n",
        "\\Rightarrow \\frac{1}{\\hat{y}_{\\theta,i}}\\frac{\\partial\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\delta_{ik}-\\hat{y}_{\\theta,k}\\\\\n",
        "\\Rightarrow \\frac{\\partial\\hat{y}_{\\theta,i}}{\\partial z_{k}} & =\\hat{y}_{\\theta,i}\\left(\\delta_{ik}-\\hat{y}_{\\theta,k}\\right).\n",
        "\\end{align*}\n",
        "\n",
        "\n",
        "Agora podemos diferenciar a entropia cruzada em relação a uma variável local $z_k$ do `softmax`.\n",
        "Sendo a entropia cruzada dada por\n",
        "$$\\mathcal{L}(y,\\hat{y}_{\\theta})  =-\\sum_{i=1}^{10}y_{i}\\ln\\hat{y}_{\\theta,i},$$\n",
        "então\n",
        "\\begin{align*}\n",
        "\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z_{k}} & =-\\sum_{i=1}^{10}y_{i}\\frac{\\partial\\ln\\hat{y}_{\\theta,i}}{\\partial z_{k}}\\\\\n",
        "& =-\\sum_{i=1}^{10}\\frac{y_{i}}{\\hat{y}_{\\theta,i}}\\frac{\\partial\\hat{y}_{\\theta,i}}{\\partial z_{k}}\\\\\n",
        "& =-\\sum_{i=1}^{10}\\frac{y_{i}}{\\hat{y}_{\\theta,i}}\\hat{y}_{\\theta,i}\\left(\\delta_{ik}-\\hat{y}_{\\theta,k}\\right)\\\\\n",
        "& =-\\sum_{i=1}^{10}y_{i}\\left(\\delta_{ik}-\\hat{y}_{\\theta,k}\\right)\\\\\n",
        "& =-\\sum_{i=1}^{10}\\left(y_{i}\\delta_{ik}+y_{i}\\hat{y}_{\\theta,k}\\right).\n",
        "\\end{align*}\n",
        "Quando $i=k$ a primeira parcela do somatório anterior se tornará $y_{k}$, reduzindo a derivada da entropia cruzada a:\n",
        "\\begin{align*}\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z_{k}} & =\\left(-y_{k}+\\hat{y}_{\\theta,k}\\sum_{i=1}^{10}y_{i}\\right).\\end{align*}\n",
        "Observando que $\\sum_{i=1}^{10}y_{i}=1$ já que $y$ é um vetor *one-hot*, obtemos então o resultado desejado:\n",
        "\\begin{align*}\n",
        "\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z_{k}} & =\\left(\\hat{y}_{\\theta,k}-y_{k}\\right).\n",
        "\\end{align*}\n",
        "\n",
        "Finalmente, na forma vetorizada (como será tratado pelo `numpy`), podemos simplesmente escrever o gradiente como\n",
        "\\begin{align*}\\frac{\\partial\\mathcal{L}(y,\\hat{y}_{\\theta})}{\\partial z} & =\\left(\\hat{y}_{\\theta}-y\\right).\\end{align*}\n"
      ],
      "metadata": {
        "id": "f9IsexmVSRfq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Finalmente, vamos mostrar como o $dW^{[2]}$ pode ser obtido.\n",
        "\n",
        "Temos que\n",
        "$$dW^{[2]}=\\frac{\\partial \\mathcal{L}}{\\partial W^{[2]}}=\\frac{\\partial \\mathcal{L}}{\\partial z^{[2]}}\\frac{\\partial z^{[2]}}{\\partial W^{[2]}}=dZ^{[2]}\\frac{\\partial z^{[2]}}{\\partial W^{[2]}}=dZ^{[2]}(A^{[1]})^{T}.$$\n",
        "\n",
        "As demais são obtidas de forma similar.\n",
        "\n",
        "*Obs.: A derivada acima é, de certa forma, intuitiva, mas a transposta parece um pouco aleatória. Para os mais curiosos, abaixo tem uma dedução para um caso simplificado mostrando o aparecimento da transposta.*\n"
      ],
      "metadata": {
        "id": "54tIDXs8iEGa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Cálculo explícito de $dW^{[2]}$"
      ],
      "metadata": {
        "id": "OzCFrR0ZwlQ2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Considere as nossas variáveis:\n",
        "\n",
        "1. A matriz de pesos $W \\in \\mathbb{R}^{2\\times 2}:$\n",
        "$$ W = \\begin{pmatrix} W_{11} & W_{12} \\\\ W_{21} & W_{22} \\end{pmatrix} $$\n",
        "2. O vetor de ativação da camada anterior $A \\in \\mathbb{R}^{2}:$\n",
        "$$ A = \\begin{pmatrix} A_1 \\\\ A_2 \\end{pmatrix} $$\n",
        "3. O vetor de viés $b \\in \\mathbb{R}^{2}$\n",
        "$$ b = \\begin{pmatrix} b_1 \\\\ b_2 \\end{pmatrix} $$\n",
        "\n",
        "A operação linear da camada é dada pela equação $\\mathbf{Z = WA + b}$. O resultado é um vetor $Z \\in \\mathbb{R}^{2}$:\n",
        "$$ Z = \\begin{pmatrix} Z_1 \\\\ Z_2 \\end{pmatrix} $$\n",
        "Expandindo a equação, obtemos os componentes escalares de $Z$:\n",
        "$$ Z_1 = W_{11}A_1 + W_{12}A_2 + b_1 \\quad \\quad (1) $$\n",
        "$$ Z_2 = W_{21}A_1 + W_{22}A_2 + b_2 \\quad \\quad (2) $$\n",
        "\n",
        "O objetivo é calcular o gradiente da função de perda $\\mathcal{L}$ (que é um escalar) em relação a cada peso na matriz $W$. O resultado será uma matriz de gradientes, $\\frac{\\partial \\mathcal{L}}{\\partial W}$, com a mesma dimensão de $W$:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W} = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial W_{11}} & \\frac{\\partial \\mathcal{L}}{\\partial W_{12}} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial W_{21}} & \\frac{\\partial \\mathcal{L}}{\\partial W_{22}} \\end{pmatrix}. $$\n",
        "*Obs.: Essa é a definição da derivada de um escalar por uma matriz.*\n",
        "\n",
        "Para este cálculo, consideramos que já conhecemos o gradiente da perda em relação a $Z$, que é o vetor:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial Z} = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\end{pmatrix} $$\n",
        "\n",
        "Agora, vamos calcular cada um dos quatro elementos da matriz de gradientes. A regra da cadeia, em sua forma completa para um peso $W_{ij}$, é:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{ij}} = \\sum_{k=1}^{2} \\frac{\\partial \\mathcal{L}}{\\partial Z_k} \\frac{\\partial Z_k}{\\partial W_{ij}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_{ij}} + \\frac{\\partial \\mathcal{L}}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_{ij}}. $$\n",
        "\n",
        "---\n",
        "**1. Cálculo para $W_{11}$**\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{11}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_{11}} + \\frac{\\partial \\mathcal{L}}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_{11}} $$\n",
        "*   Analisando a derivada $\\frac{\\partial Z_1}{\\partial W_{11}}$: Usando a Equação (1), temos que $\\frac{\\partial}{\\partial W_{11}}(W_{11}A_1 + W_{12}A_2 + b_1) = A_1$.\n",
        "*   Analisando a derivada $\\frac{\\partial Z_2}{\\partial W_{11}}$: Usando a Equação (2), o termo $W_{11}$ não aparece, então a derivada é $0$.\n",
        "\n",
        "Substituindo os resultados:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{11}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\cdot A_1 + \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\cdot 0 = \\frac{\\partial \\mathcal{L}}{\\partial Z_1} A_1 $$\n",
        "\n",
        "---\n",
        "**2. Cálculo para $W_{12}$**\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{12}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_{12}} + \\frac{\\partial \\mathcal{L}}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_{12}} $$\n",
        "*   Analisando $\\frac{\\partial Z_1}{\\partial W_{12}}$: Da Equação (1), a derivada é $A_2$.\n",
        "*   Analisando $\\frac{\\partial Z_2}{\\partial W_{12}}$: Da Equação (2), a derivada é $0$.\n",
        "\n",
        "Substituindo os resultados:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{12}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\cdot A_2 + \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\cdot 0 = \\frac{\\partial \\mathcal{L}}{\\partial Z_1} A_2 $$\n",
        "\n",
        "---\n",
        "**3. Cálculo para $W_{21}$**\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{21}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_{21}} + \\frac{\\partial \\mathcal{L}}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_{21}} $$\n",
        "*   Analisando $\\frac{\\partial Z_1}{\\partial W_{21}}$: Da Equação (1), a derivada é $0$.\n",
        "*   Analisando $\\frac{\\partial Z_2}{\\partial W_{21}}$: Da Equação (2), a derivada é $A_1$.\n",
        "\n",
        "Substituindo os resultados:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{21}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\cdot 0 + \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\cdot A_1 = \\frac{\\partial \\mathcal{L}}{\\partial Z_2} A_1 $$\n",
        "\n",
        "---\n",
        "**4. Cálculo para $W_{22}$**\n",
        "\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{22}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1}\\frac{\\partial Z_1}{\\partial W_{22}} + \\frac{\\partial \\mathcal{L}}{\\partial Z_2}\\frac{\\partial Z_2}{\\partial W_{22}} $$\n",
        "*   Analisando $\\frac{\\partial Z_1}{\\partial W_{22}}$: Da Equação (1), a derivada é $0$.\n",
        "*   Analisando $\\frac{\\partial Z_2}{\\partial W_{22}}$: Da Equação (2), a derivada é $A_2$.\n",
        "\n",
        "Substituindo os resultados:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W_{22}} = \\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\cdot 0 + \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\cdot A_2 = \\frac{\\partial \\mathcal{L}}{\\partial Z_2} A_2 $$\n",
        "\n",
        "---\n",
        "Finalmente, vamos agora reunir os quatro resultados escalares para construir a matriz de gradientes $\\frac{\\partial \\mathcal{L}}{\\partial W}$:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial W} = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z_1} A_1 & \\frac{\\partial \\mathcal{L}}{\\partial Z_1} A_2 \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z_2} A_1 & \\frac{\\partial \\mathcal{L}}{\\partial Z_2} A_2 \\end{pmatrix} $$\n",
        "Agora, vamos analisar a operação de produto externo (que é um caso de multiplicação de matrizes) entre o vetor $\\frac{\\partial \\mathcal{L}}{\\partial Z}$ e a transposta do vetor $A$, denotada por $A^T$. Temos que o produto abaixo não faz sentido do ponto de vista da multiplicação de matrizes usual\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial Z} A^T = \\begin{pmatrix} \\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\\\ \\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\end{pmatrix} \\begin{pmatrix} A_1 & A_2 \\end{pmatrix}, $$\n",
        "porém do ponto de vista do produto externo, o produto de um vetor coluna de $2 \\times 1$ por um vetor linha de $1 \\times 2$ resulta em uma matriz de $2 \\times 2$ na forma:\n",
        "$$ \\frac{\\partial \\mathcal{L}}{\\partial Z} A^T = \\begin{pmatrix} (\\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\cdot A_1) & (\\frac{\\partial \\mathcal{L}}{\\partial Z_1} \\cdot A_2) \\\\ (\\frac{\\partial \\mathcal{L}}{\\partial Z_2} \\cdot A_1) & (\\frac{\\partial L}{\\partial Z_2} \\cdot A_2) \\end{pmatrix} $$\n",
        "que corresponde exatamente a nossa derivada: $\\frac{\\partial \\mathcal{L}}{\\partial W}.$"
      ],
      "metadata": {
        "id": "-UdvtTVGsIdp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Código das derivadas\n",
        "---"
      ],
      "metadata": {
        "id": "PMY4c3_aw3xO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# cálculo das derivadas\n",
        "\n",
        "def backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y):\n",
        "    one_hot_Y = one_hot(Y)\n",
        "    # aqui dividimos pelo número de imagens m, pois a loss calculada é a média das amostras\n",
        "    dZ2 = (1 / m) * (A2 - one_hot_Y)\n",
        "\n",
        "    dW2 = dZ2.dot(A1.T)\n",
        "    db2 = np.sum(dZ2, axis=1).reshape(-1, 1)\n",
        "    dZ1 = W2.T.dot(dZ2) * ReLU_deriv(Z1)\n",
        "    dW1 = dZ1.dot(X.T)\n",
        "    db1 = np.sum(dZ1, axis=1).reshape(-1, 1)\n",
        "    return dW1, db1, dW2, db2"
      ],
      "metadata": {
        "id": "0tG6Wj8sjZU-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Atualiza os parâmetros de acordo com as derivadas. Faz a atualização de acordo com o gradiente descendente."
      ],
      "metadata": {
        "id": "uuRSPDmhdjpJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# atualização dos parâmetros\n",
        "def update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha):\n",
        "    W1 -= alpha * dW1\n",
        "    b1 -= alpha * db1\n",
        "    W2 -= alpha * dW2\n",
        "    b2 -= alpha * db2\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "jYynRIIEk4i4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### Parte 2.2: definição da função que fará as épocas\n",
        "Efetivamente a função que faz o processo de cálculo da rede neural acontecer e atualiza os parâmetros sucessivamente"
      ],
      "metadata": {
        "id": "P36H_z9WduiC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos fazer a mágica acontecer!\n",
        "\n",
        "def get_predictions(A2):\n",
        "    return np.argmax(A2, 0)\n",
        "\n",
        "def get_accuracy(predictions, Y):\n",
        "    print(predictions, Y)\n",
        "    return np.sum(predictions == Y) / Y.size\n",
        "\n",
        "def gradient_descent(X, Y, alpha, iterations):\n",
        "    W1, b1, W2, b2 = init_params()\n",
        "    for i in range(iterations):\n",
        "        Z1, A1, Z2, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "        dW1, db1, dW2, db2 = backward_prop(Z1, A1, Z2, A2, W1, W2, X, Y)\n",
        "        W1, b1, W2, b2 = update_params(W1, b1, W2, b2, dW1, db1, dW2, db2, alpha)\n",
        "        if i % 50 == 0:\n",
        "            print(\"Iteração: \", i)\n",
        "            predictions = get_predictions(A2)\n",
        "            print(get_accuracy(predictions, Y))\n",
        "    return W1, b1, W2, b2"
      ],
      "metadata": {
        "id": "t44TOFJXl7iK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## Parte 3: treinamento e análise dos resultados"
      ],
      "metadata": {
        "id": "2hf_bYIMqZye"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Finalmente realiza o treinamento!"
      ],
      "metadata": {
        "id": "HIfJgv7Ad5xs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos treinar\n",
        "W1, b1, W2, b2 = gradient_descent(X_train, Y_train, 0.10, 750)"
      ],
      "metadata": {
        "id": "xYzkXrtvnAfd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e837f05b-856d-42bc-8d54-4b1e2118c0df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteração:  0\n",
            "[1 2 2 ... 1 8 1] [9 2 3 ... 6 3 7]\n",
            "0.133994708994709\n",
            "Iteração:  50\n",
            "[4 2 2 ... 6 4 9] [9 2 3 ... 6 3 7]\n",
            "0.41365079365079366\n",
            "Iteração:  100\n",
            "[4 2 2 ... 6 8 7] [9 2 3 ... 6 3 7]\n",
            "0.5897619047619047\n",
            "Iteração:  150\n",
            "[9 2 2 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.697962962962963\n",
            "Iteração:  200\n",
            "[9 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.7564814814814815\n",
            "Iteração:  250\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.787063492063492\n",
            "Iteração:  300\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8065343915343915\n",
            "Iteração:  350\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8196296296296296\n",
            "Iteração:  400\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8311111111111111\n",
            "Iteração:  450\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8391798941798941\n",
            "Iteração:  500\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8458201058201058\n",
            "Iteração:  550\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.850925925925926\n",
            "Iteração:  600\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8557142857142858\n",
            "Iteração:  650\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8598412698412699\n",
            "Iteração:  700\n",
            "[4 2 8 ... 6 3 7] [9 2 3 ... 6 3 7]\n",
            "0.8632804232804233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Funções para verificar a qualidade da nossa rede neural"
      ],
      "metadata": {
        "id": "xJHjqO1VeBNc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# vamos testar se o trem ficou bom!\n",
        "def make_predictions(X, W1, b1, W2, b2):\n",
        "    _, _, _, A2 = forward_prop(W1, b1, W2, b2, X)\n",
        "    predictions = get_predictions(A2)\n",
        "    return predictions\n",
        "\n",
        "def test_prediction(index, W1, b1, W2, b2):\n",
        "    current_image = X_dev[:, index, None]\n",
        "    prediction = make_predictions(X_dev[:, index, None], W1, b1, W2, b2)\n",
        "    label = Y_dev[index]\n",
        "    print(\"Previsão: \", prediction)\n",
        "    print(\"Rótulo: \", label)\n",
        "\n",
        "    current_image = current_image.reshape((28, 28))\n",
        "    plt.imshow(current_image,cmap=\"gray\")\n",
        "    plt.axis('off')\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "BkHK3dnHor9p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "Testa o quanto a nossa rede acerta!"
      ],
      "metadata": {
        "id": "B13Ye0_4eJr1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a, b = X_dev.shape\n",
        "\n",
        "test_prediction(np.random.randint(1, b), W1, b1, W2, b2)"
      ],
      "metadata": {
        "id": "3icE-RtupKs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "outputId": "ed280bae-ff15-4ad1-d459-aefbfcce8780"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Previsão:  [3]\n",
            "Rótulo:  5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}